---
title: "Fundamentals of Bayesian Data Analysis in R"
author: "Hamed Bastan-Hagh"
output: 
  html_document: 
    highlight: kate
    theme: journal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Notes from DataCamp course [Fundamentals of Bayesian Data Analysis in R][1].

```{r}
library(tidyverse)
```


# Chapter One: What is Bayesian Data Analysis?

Focus is on binomial setup. Use a helper function, `prop_model()`, to visualise how the posterior distribution changes given new data.

```{r}
prop_model <- function (data = c(), 
                        prior_prop = c(1, 1), 
                        n_draws = 10000, 
                        show_plot = TRUE) {
    data <- as.logical(data)
    proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
    data_indices <- round(seq(0, 
                              length(data), 
                              length.out = min(length(data) + 1, 
                                               20)))
    post_curves <- purrr::map_dfr(
        data_indices, 
        function(i) {
            value <- ifelse(i == 0, 
                            "Prior", 
                            ifelse(data[i], 
                                   "Success", 
                                   "Failure"))
            label <- paste0("n=", i)
            probability <- dbeta(proportion_success, 
                                 prior_prop[1] + 
                                     sum(data[seq_len(i)]), 
                                 prior_prop[2] + 
                                     sum(!data[seq_len(i)]))
            probability <- probability/max(probability)
            dplyr::data_frame(value, 
                              label, 
                              proportion_success, 
                              probability)
        })
    post_curves$label <- forcats::fct_rev(factor(post_curves$label, 
                                                 levels = paste0("n=", 
                                                                 data_indices)))
    post_curves$value <- factor(post_curves$value, 
                                levels = c("Prior", "Success", "Failure"))
    p <- ggplot2::ggplot(post_curves, 
                         ggplot2::aes(x = proportion_success, 
                                      y = label, 
                                      height = probability, 
                                      fill = value)) + 
        ggridges::geom_density_ridges(stat = "identity", 
                                      color = "white", 
                                      alpha = 0.8, 
                                      panel_scaling = TRUE, 
                                      size = 1) + 
        ggplot2::scale_y_discrete("", expand = c(0.01, 0)) + 
        ggplot2::scale_x_continuous("Underlying proportion of success") + 
        ggplot2::scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65),
                                   name = "", 
                                   drop = FALSE, 
                                   labels = c("Prior   ", 
                                              "Success   ", 
                                              "Failure   ")) +
        ggplot2::theme_light(base_size = 18) +
        ggplot2::theme(legend.position = "top")
    if (show_plot) {
        print(p)
    }
    invisible(rbeta(n_draws, 
                    prior_prop[1] + sum(data), 
                    prior_prop[2] + sum(!data)))
}
```

Run this on a simple set of results.

```{r}
prop_model(data = c(1, 1, 0, 0))
```

Try something a bit more complex.

```{r}
set.seed(1711)
prop_model(data = sample(c(0, 1), size = 12, replace = TRUE))
```

One example is a drug that might prevent people turning into zombies. Suppose it works for two of thirteen people, what would the posterior look like? Gives an example to show that order of updating doesn't matter.

```{r}
zombie_data <- c(rep(0, 11), 1, 1)
set.seed(123)
prop_model(sample(zombie_data))
set.seed(124)
prop_model(sample(zombie_data))
```

Distributions end up identical.

The function also returns a vector of samples from the posterior distribution, which we can use for plotting.

```{r}
set.seed(125)
zombie_prop <- prop_model(sample(zombie_data))
tibble(p = zombie_prop) %>% 
    ggplot(aes(x = p)) + 
    geom_histogram(bins = 30, fill = "palegreen4") + 
    theme_light()
```

# Chapter Two: How Does Bayesian Inference Work?

Three elements needed for a Bayesian model:

1. Data;
2. Generative Model;
3. Priors.

## What is a Generative Model?

A computer programme, mathematical expression, or set of rules into which we can feed fixed parameter values and generate data. Can use an example with the zombies, for which we need two parameters: 

1. The proportion of zombies cured by the drug (`prop_success`);
2. The total number of zombies (`n_zombies`).

```{r}
# start by setting values for the parameters
prop_success <- 0.15
n_zombies <- 13

# Simulate some data, assuming that the outcome depends only on prop_success
sum(runif(n = n_zombies) < prop_success)

# Can run this simulation many times to get different counts
replicate(100, sum(runif(n = n_zombies) < prop_success))
```

This is only doing what `rbinom()` does.

```{r}
rbinom(n = 100, size = 13, prob = 0.15)
```

This allows for generating data from known parameters. But typically it's the reverse that's true: we have data and the parameters are unknown, so we need to estimate them.

Switch to a more realistic example: when considering placing online adverts on a social media platform, success is now defined as the person clicking on our ad. The social media company, Wastebook, says that 10% of ads get clicked. We can simulate this and visualise the result.

```{r}
# Run the simulation 100,000 times and plot a histogram
click_counts <- rbinom(1e5, 100, 0.1)
tibble(clicks = click_counts) %>% 
    ggplot(aes(x = clicks)) + 
    geom_histogram(binwidth = 1) + 
    scale_x_continuous(breaks = seq(0, max(click_counts), by = 5)) + 
    theme_light()
```

## Prior Probability Distribution

The prior distributions for parameters should reflect our uncertainty. We can set the number of adverts (in this case, 100) ourselves, so there is little uncertainty. But a click-through rate of 10% is both high and a conveniently round number! Suppose we think the true value of the click-through rate $p \in [0, 0.2]$. We can then simulate from that instead.

```{r}
# Generate values for p, then generate the click counts again.
click_probs <- runif(1e5, min = 0, max = 0.2)
click_counts <- rbinom(1e5, 100, click_probs)
tibble(clicks = click_counts) %>% 
    ggplot(aes(x = clicks)) + 
    geom_histogram(binwidth = 1) + 
    scale_x_continuous(breaks = seq(0, max(click_counts), by = 5)) + 
    theme_light()
```

Now suppose we go ahead with the advert, and get 13 clicks on 100 ads. What does that tell us about $p$?

```{r}
# Build tibble for the priors
prior <- tibble(prob = click_probs, 
                clicks = click_counts)
head(prior)

# Posterior tibble is just those that fit the results, i.e. 13 clicks
posterior <- prior %>% 
    filter(clicks == 13)
head(posterior)

# Visualise the distribution of p in the posterior  
posterior %>% 
    ggplot(aes(x = prob)) + 
    geom_histogram(bins = 20) + 
    theme_light()
```

Now we can sample from this distribution of probabilities to get an idea of how the advert might perform in the future.

```{r}
forecast <- posterior %>% 
    dplyr::select(prob) %>% 
    mutate(clicks = map_int(prob, 
                            ~ rbinom(1, 100, .x)))
forecast %>% 
    ggplot(aes(x = clicks)) + 
    geom_histogram(binwidth = 1) + 
    scale_x_continuous(breaks = seq(0, max(forecast$clicks), by = 5)) + 
    theme_light()
```

We can use `forecast` to answer questions: how likely are we to get more than 5, 10, 15 clicks?

```{r}
map_dbl(c(5, 10, 15), 
        ~ mean(forecast$clicks >= .x))
```

Can repeat this with the prior that $p \sim \text{Beta}(2, 8)$, as plotted below.

```{r}
curve(dbeta(x, 2, 8), 0, 1)
```

```{r}
click_probs <- rbeta(1e5, shape1 = 2, shape2 = 8)
click_counts <- rbinom(1e5, 100, click_probs)
tibble(clicks = click_counts) %>% 
    ggplot(aes(x = clicks)) + 
    geom_histogram(binwidth = 1) + 
    scale_x_continuous(breaks = seq(0, max(click_counts), by = 5)) + 
    theme_light()
```

How does this fit with the results of 13 clicks?

```{r}
# Build tibble for the priors
prior <- tibble(prob = click_probs, 
                clicks = click_counts)
head(prior)

# Posterior tibble is just those that fit the results, i.e. 13 clicks
posterior <- prior %>% 
    filter(clicks == 13)
head(posterior)

# Visualise the distribution of p in the posterior  
posterior %>% 
    ggplot(aes(x = prob)) + 
    geom_histogram(bins = 20) + 
    theme_light()
```

```{r}
forecast <- posterior %>% 
    dplyr::select(prob) %>% 
    mutate(clicks = map_int(prob, 
                            ~ rbinom(1, 100, .x)))
forecast %>% 
    ggplot(aes(x = clicks)) + 
    geom_histogram(binwidth = 1) + 
    scale_x_continuous(breaks = seq(0, max(forecast$clicks), by = 5)) + 
    theme_light()
```

```{r}
map_dbl(c(5, 10, 15), 
        ~ mean(forecast$clicks >= .x))
```


[1]: https://www.datacamp.com/courses/fundamentals-of-bayesian-data-analysis-in-r